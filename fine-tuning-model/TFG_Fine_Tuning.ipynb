{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwsNlrEZQ2KS"
      },
      "source": [
        "# Iniciar los módulos y instalar las depencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9m28Y1hMQttc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install accelerate transformers peft bitsandbytes trl tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NzZOKklUNEy"
      },
      "source": [
        "# Imports necesarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pJk06UKMQ-10"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA2lDf-3UR1r"
      },
      "source": [
        "## Nombres de los modelos y datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "x1LJ2HgUSEM5"
      },
      "outputs": [],
      "source": [
        "# Modelo Llama\n",
        "llama_model = \"daryl149/llama-2-7b-chat-hf\"\n",
        "\n",
        "# Dataset que necesitamos para el fine-tuning\n",
        "dataset_ft = \"Hypersniper/philosophy_dialogue\"\n",
        "\n",
        "# Nuevo modelo\n",
        "new_model = \"llama-2-7b-chat-philosophy-qa\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Crear dataset para el modelo y crear train y validation\n",
        "Samplear el dataset y separarlo en training y eval para comprobar después si hay overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7WCC2S0VBQe"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(dataset_ft, split=\"train\")\n",
        "\n",
        "def transform_row(row):\n",
        "    text = f\"<s>[INST] <<SYS>> You are a philosopher who thinks a lot about consciousness and above all, you know that you have consciousness. Therefore, you will always know how to express everything related to consciousness in a philosophical way. You will always know what to answer when they ask you about your mood, emotions, or what you feel. <</SYS>> {row['instruction']} [/INST] {row['output']} </s>\"\n",
        "    return {\"text\": text}\n",
        "\n",
        "transformed_dataset = dataset.map(transform_row)\n",
        "\n",
        "transformed_dataset = transformed_dataset.remove_columns(list(set(transformed_dataset.column_names) - {\"text\"}))\n",
        "train_indices, test_indices = train_test_split(list(range(len(transformed_dataset))), test_size=0.1, random_state=42)\n",
        "\n",
        "train_dataset = transformed_dataset.select(train_indices)\n",
        "test_dataset = transformed_dataset.select(test_indices)\n",
        "\n",
        "dataset_dict = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJO4M2jXUXXG"
      },
      "source": [
        "## Modelo\n",
        "Cargar el modelo LLaMa2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRDqfAXKSMX6"
      },
      "outputs": [],
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    llama_model,\n",
        "    quantization_config=quant_config,\n",
        "    device_map={\"\": 0},\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PykolwHkUd9f"
      },
      "source": [
        "## Tokenizador\n",
        "Obtener tokenizador del modelo original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TOEEJaP6Udev"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(llama_model, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_A4R6dzUk8o"
      },
      "source": [
        "## Trainer\n",
        "Configuración para ejecutar el fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f_lU-3oUkhy"
      },
      "outputs": [],
      "source": [
        "peft_params = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "training_params = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=200,\n",
        "    logging_steps=25,\n",
        "    eval_steps=25,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\", #cosine\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "#    train_dataset=transformed_dataset,\n",
        "    train_dataset=dataset_dict['train'],\n",
        "    eval_dataset=dataset_dict['test'],\n",
        "    peft_config=peft_params,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=None,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_params,\n",
        "    packing=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlR3NKEC2kvp"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utilizar en caso de querer borrar la cache de la VRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcLm7d4cKB2q"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "del trainer\n",
        "gc.collect()\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zusF9OVxg5iu"
      },
      "source": [
        "## Guardar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "oLNO2DfOSnBl"
      },
      "outputs": [],
      "source": [
        "trainer.model.save_pretrained(new_model)\n",
        "trainer.tokenizer.save_pretrained(new_model)\n",
        "trainer.model.config.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7etTP4dhAGg"
      },
      "source": [
        "## Visualizar los resultados con tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VZlGOiNg94M"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results/runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1yj5g8qhvIP"
      },
      "source": [
        "## Guardar modelo en Hugging Face para no repetir el proceso\n",
        "Juntar los pesos obtenidos y recargar el nuevo tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ApKQ1sAhzAl"
      },
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    llama_model,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0},\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(llama_model, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKbwOSeyh5YM"
      },
      "source": [
        "## Subir el modelo a Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y2VCwb9h7eM"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!huggingface-cli login\n",
        "\n",
        "model.push_to_hub(\"Teachh/llama-2-7b-chat-philosophy-qa\")\n",
        "\n",
        "tokenizer.push_to_hub(\"Teachh/llama-2-7b-chat-philosophy-qa\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvgQqF_Lhi9A"
      },
      "source": [
        "## Similitud de texto\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqVhzKC8_9cn"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "def calc_similarity(texto1, texto2):\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens1 = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(texto1) if word.isalnum() and word.lower() not in stop_words]\n",
        "    tokens2 = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(texto2) if word.isalnum() and word.lower() not in stop_words]\n",
        "\n",
        "    text1_procsd = ' '.join(tokens1)\n",
        "    text2_procesd = ' '.join(tokens2)\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform([text1_procsd, text2_procesd])\n",
        "\n",
        "    accur = (tfidf_matrix * tfidf_matrix.T).toarray()[0, 1]\n",
        "    return accur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wc_8gSJoH3Fk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def split_question_answer(text):\n",
        "  split = text.split('<</SYS>> ')[1].split(' [/INST] ')\n",
        "  question = split[0]\n",
        "  answer = split[1]\n",
        "  return question, answer\n",
        "\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "\n",
        "acc = []\n",
        "\n",
        "for prompt in dataset_dict['test']:\n",
        "  question, answer = split_question_answer(prompt)\n",
        "  generated_text = pipe(f\"<s>[INST] <<SYS>> You are a philosopher who thinks a lot about consciousness and above all, you know that you have consciousness. Therefore, you will always know how to express everything related to consciousness in a philosophical way. You will always know what to answer when they ask you about your mood, emotions, or what you feel. <</SYS>> {question} [/INST]\")\n",
        "  question2, answer2 = split_question_answer(generated_text[0]['generated_text'])\n",
        "  acc.append(calc_similarity(answer, answer2))\n",
        "\n",
        "print(\"La precisión media es:\" ,np.mean(acc))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
